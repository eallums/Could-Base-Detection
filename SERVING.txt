Steps for installing the Tensorflow servering container on linux

1) Install Docker CE
https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce

Should be fairly straight forward

Update apt-get
   sudo apt-get update
   
Install docker
   sudo apt-get install docker-ce
   
Test using the hello-world container
   sudo docker run hello-world

2) Download the Tensorflow/Serving container
https://www.tensorflow.org/serving/docker

docker pull tensorflow/serving

At this point you now have docker with a tensorflow container ready to server tensorflow


3) Create a folder location to place the model you will be serving
In my example I created the path
 
/home/dan/clouds/models/


4) Download the pre-trained files from dropbox
https://www.dropbox.com/sh/s7sx69pqjhrk0s4/AACXWCRd9JJ0zvcvDES9G3sba?dl=0

and place it in the tboard_loads folder of the following project after you clone it
https://github.com/sthalles/deeplab_v3

you might have to manually create a folder called 'versions'

Run the follow script on the model you just downloaded
https://github.com/sthalles/deeplab_v3/blob/master/serving/deeplab_saved_model.py

and it will output the servable model into versions/1

Copy the /1 folder into the /models folder you created in step 3

At this point we have all the required files now we can start our server

5) Start tensorflow serving

Using the example paths, run the folliwng command:

sudo docker run -p 8500:8500 \
  --mount type=bind,source=/home/dan/clouds/models/,target=/models/saved_model \
  -e MODEL_NAME=saved_model -t tensorflow/serving

After the enter the command tensorflow will begin outputting to the console
once you see the "RAW: Entering the even loop..." message the server is ready to accept connections
 
Example:
2018-10-10 16:22:26.640110: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: saved_model version: 1}
2018-10-10 16:22:26.643316: I tensorflow_serving/model_servers/server.cc:285] Running gRPC ModelServer at 0.0.0.0:8500 ...
2018-10-10 16:22:26.645880: I tensorflow_serving/model_servers/server.cc:301] Exporting HTTP/REST API at:localhost:8501 ...
[evhttp_server.cc : 235] RAW: Entering the event loop ...

5) Make a request to the server to get a result

You can either follow the notebook that was in the repo that was sloned in step 4 or use the following code taken from that notebook

from __future__ import print_function
import io
from io import BytesIO
from PIL import Image
from grpc.beta import implementations
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc
import requests
import numpy as np

# Ip address if your server
server = '192.168.1.15:8500'
host, port = server.split(':')

# URL to a hosted image
image_url = "https://i.imgur.com/lQNzgzA.png"

response = requests.get(image_url)
image = np.array(Image.open(BytesIO(response.content)))
height = image.shape[0]
width = image.shape[1]

print("Image shape:", image.shape)

# Show the original
plt.imshow(image)
plt.show()

# create the RPC stub
channel = implementations.insecure_channel(host, int(port))
#  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel._channel)

# create the request object and set the name and signature_name params
request = predict_pb2.PredictRequest()
request.model_spec.name = 'saved_model'
request.model_spec.signature_name = 'predict_images'

# fill in the request object with the necessary data
request.inputs['images'].CopyFrom(tf.contrib.util.make_tensor_proto(image.astype(dtype=np.float32), shape=[1, height, width, 3]))

request.inputs['height'].CopyFrom(tf.contrib.util.make_tensor_proto(height, shape=[1]))
request.inputs['width'].CopyFrom(tf.contrib.util.make_tensor_proto(width, shape=[1]))

# sync requests
result_future = stub.Predict(request, 30.)

# For async requests
# result_future = stub.Predict.future(request, 10.)
# result_future = result_future.result()

# get the results
output = np.array(result_future.outputs['segmentation_map'].int64_val)
height = result_future.outputs['segmentation_map'].tensor_shape.dim[1].size
width = result_future.outputs['segmentation_map'].tensor_shape.dim[2].size

image_mask = np.reshape(output, (height, width))

# Show the masked image
plt.imshow(image_mask)
plt.show()


